{"cells":[{"cell_type":"markdown","id":"b821e683","metadata":{},"source":["By now you know that there are two main types of machine learning: supervised and unsupervised. And you are also familiar with one of the most popular supervised learning models, linear regression, which is especially helpful for predicting numerical values, like the price of a house or the number of umbrellas. But what if we are not interested in a number, but a category?"]},{"cell_type":"markdown","id":"9087fde2","metadata":{},"source":["Classification is the process of categorizing a given set of data into classes. Here, the classes act as our labels, or ground truth. A classification model uses the features of an object to predict its labels. As we say labels, you can probably guess that here we have another supervised learning model at hand. \n","\n"," The algorithm used by your email service providers to filter spam from non-spam emails is an example of classification. This model uses the features of the email: subject, sender’s email address, email body, and attachments as inputs; and makes a prediction for one out of the two classes: spam or non-spam. This is an example of binary classification, \n"," \n"," where the output is restricted to two classes. Spam and non-spam, true and false, zeros and ones, yes and no, positive, or negative and so on. \n"," \n"," If there are more than two classes, we have a multi-class classification problem. An example of multi-class classification can be classifying types of fruits based on their color, weight, and size. Or movies into different genres like comedy, romance, drama, and horror."]},{"cell_type":"markdown","id":"a2505f15","metadata":{},"source":["The question is, how can machine learning solve this problem? Let’s start with our first classification model: Logistic regression. The best way to think about logistic regression is that it is a linear regression but for classification problems. Logistic regression uses a logistic function, specifically the Sigmoid function. This function takes any real input, and outputs a value between zero and one. Unlike linear regression, logistic regression doesn’t need a linear relationship between input and output variables. Once we have the predicted results from our classification model, or classifier, we compare these results with the actual label, the ground truth, and evaluate the performance of our model."]},{"cell_type":"markdown","id":"ea76b7b6","metadata":{},"source":["We categorize each prediction into four categories: True positives, False positives, True negatives, and False negatives. True positives are the results which were predicted as positive and ground truth were also positive. False positives are the instances which were predicted as positives but actually they were negative. Likewise, True negatives are the instances which were predicted as negatives and their ground truth was also negative. And False negatives are the instances which were predicted as negative, but their ground truth was positive. This is called the confusion matrix. We want our true positives and negatives to be maximized and false positives and negatives to be minimized. We use these categories to define our evaluation metrics: accuracy, precision, recall and F1 score.\n","\n","The accuracy of an algorithm is represented as the ratio of the correctly classified instances and the total number of samples. The precision of an algorithm is represented as the ratio of correctly classified instances with the positive class to the total samples that are predicted positive. The recall metric is defined as the ratio of correctly classified positive class divided by total number of instances which are actually positive. The idea behind recall is to know how many positive samples the classifier has mis predicted. Recall is also called sensitivity.\n","\n","The F1 score is also known as the F Measure. It indicates the equilibrium between the precision and the recall. Let’s see how this plays out in practice."]},{"cell_type":"markdown","id":"e900d5c6","metadata":{},"source":["We have a binary classification problem. We need to classify tumors into malignant ones that are cancerous or benign which means non-cancerous. Our dataset contains statistical data from histopathology examinations. We will use this dataset to train our logistic regression model."]},{"cell_type":"markdown","id":"ecafad84","metadata":{},"source":[" Let’s import pandas and read the dataset. "]},{"cell_type":"code","execution_count":null,"id":"6a2bceed","metadata":{"id":"6a2bceed"},"outputs":[],"source":["import pandas as pd\n","dataset = pd.read_csv(\"breast-cancer.csv\")"]},{"cell_type":"markdown","id":"cbb33d66","metadata":{},"source":["Using the shape method, we can easily get the number of observations and features. In this dataset, there are unique instances. The features are radius mean, texture mean, radius worst, and so on. There are features in total. "]},{"cell_type":"code","execution_count":null,"id":"bf1c090b","metadata":{"id":"bf1c090b","outputId":"522acbe6-b426-45f5-c188-c272df399b7f"},"outputs":[{"data":{"text/plain":["(569, 31)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset.shape"]},{"cell_type":"markdown","id":"69b708cd","metadata":{},"source":["We can check the first 5 instances in our dataset using the head function. Here, we can see, the first column represents the target variable. The following columns are the features."]},{"cell_type":"code","execution_count":null,"id":"272cd872","metadata":{"id":"272cd872","outputId":"8a6f9bfa-0336-4380-8a01-d0f506b70000"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>M</td>\n","      <td>1.097064</td>\n","      <td>-2.073335</td>\n","      <td>1.269934</td>\n","      <td>0.984375</td>\n","      <td>1.568466</td>\n","      <td>3.283515</td>\n","      <td>2.652874</td>\n","      <td>2.532475</td>\n","      <td>2.217515</td>\n","      <td>...</td>\n","      <td>1.886690</td>\n","      <td>-1.359293</td>\n","      <td>2.303601</td>\n","      <td>2.001237</td>\n","      <td>1.307686</td>\n","      <td>2.616665</td>\n","      <td>2.109526</td>\n","      <td>2.296076</td>\n","      <td>2.750622</td>\n","      <td>1.937015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>M</td>\n","      <td>1.829821</td>\n","      <td>-0.353632</td>\n","      <td>1.685955</td>\n","      <td>1.908708</td>\n","      <td>-0.826962</td>\n","      <td>-0.487072</td>\n","      <td>-0.023846</td>\n","      <td>0.548144</td>\n","      <td>0.001392</td>\n","      <td>...</td>\n","      <td>1.805927</td>\n","      <td>-0.369203</td>\n","      <td>1.535126</td>\n","      <td>1.890489</td>\n","      <td>-0.375612</td>\n","      <td>-0.430444</td>\n","      <td>-0.146749</td>\n","      <td>1.087084</td>\n","      <td>-0.243890</td>\n","      <td>0.281190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>1.579888</td>\n","      <td>0.456187</td>\n","      <td>1.566503</td>\n","      <td>1.558884</td>\n","      <td>0.942210</td>\n","      <td>1.052926</td>\n","      <td>1.363478</td>\n","      <td>2.037231</td>\n","      <td>0.939685</td>\n","      <td>...</td>\n","      <td>1.511870</td>\n","      <td>-0.023974</td>\n","      <td>1.347475</td>\n","      <td>1.456285</td>\n","      <td>0.527407</td>\n","      <td>1.082932</td>\n","      <td>0.854974</td>\n","      <td>1.955000</td>\n","      <td>1.152255</td>\n","      <td>0.201391</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>-0.768909</td>\n","      <td>0.253732</td>\n","      <td>-0.592687</td>\n","      <td>-0.764464</td>\n","      <td>3.283553</td>\n","      <td>3.402909</td>\n","      <td>1.915897</td>\n","      <td>1.451707</td>\n","      <td>2.867383</td>\n","      <td>...</td>\n","      <td>-0.281464</td>\n","      <td>0.133984</td>\n","      <td>-0.249939</td>\n","      <td>-0.550021</td>\n","      <td>3.394275</td>\n","      <td>3.893397</td>\n","      <td>1.989588</td>\n","      <td>2.175786</td>\n","      <td>6.046041</td>\n","      <td>4.935010</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>M</td>\n","      <td>1.750297</td>\n","      <td>-1.151816</td>\n","      <td>1.776573</td>\n","      <td>1.826229</td>\n","      <td>0.280372</td>\n","      <td>0.539340</td>\n","      <td>1.371011</td>\n","      <td>1.428493</td>\n","      <td>-0.009560</td>\n","      <td>...</td>\n","      <td>1.298575</td>\n","      <td>-1.466770</td>\n","      <td>1.338539</td>\n","      <td>1.220724</td>\n","      <td>0.220556</td>\n","      <td>-0.313395</td>\n","      <td>0.613179</td>\n","      <td>0.729259</td>\n","      <td>-0.868353</td>\n","      <td>-0.397100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0         M     1.097064     -2.073335        1.269934   0.984375   \n","1         M     1.829821     -0.353632        1.685955   1.908708   \n","2         M     1.579888      0.456187        1.566503   1.558884   \n","3         M    -0.768909      0.253732       -0.592687  -0.764464   \n","4         M     1.750297     -1.151816        1.776573   1.826229   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0         1.568466          3.283515        2.652874             2.532475   \n","1        -0.826962         -0.487072       -0.023846             0.548144   \n","2         0.942210          1.052926        1.363478             2.037231   \n","3         3.283553          3.402909        1.915897             1.451707   \n","4         0.280372          0.539340        1.371011             1.428493   \n","\n","   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","0       2.217515  ...      1.886690      -1.359293         2.303601   \n","1       0.001392  ...      1.805927      -0.369203         1.535126   \n","2       0.939685  ...      1.511870      -0.023974         1.347475   \n","3       2.867383  ...     -0.281464       0.133984        -0.249939   \n","4      -0.009560  ...      1.298575      -1.466770         1.338539   \n","\n","   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","0    2.001237          1.307686           2.616665         2.109526   \n","1    1.890489         -0.375612          -0.430444        -0.146749   \n","2    1.456285          0.527407           1.082932         0.854974   \n","3   -0.550021          3.394275           3.893397         1.989588   \n","4    1.220724          0.220556          -0.313395         0.613179   \n","\n","   concave points_worst  symmetry_worst  fractal_dimension_worst  \n","0              2.296076        2.750622                 1.937015  \n","1              1.087084       -0.243890                 0.281190  \n","2              1.955000        1.152255                 0.201391  \n","3              2.175786        6.046041                 4.935010  \n","4              0.729259       -0.868353                -0.397100  \n","\n","[5 rows x 31 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"id":"395ced3c","metadata":{"id":"395ced3c","outputId":"1e2dca05-d8a3-4467-f2ff-7bd31c56a648","scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>564</th>\n","      <td>M</td>\n","      <td>2.110995</td>\n","      <td>0.721473</td>\n","      <td>2.060786</td>\n","      <td>2.343856</td>\n","      <td>1.041842</td>\n","      <td>0.219060</td>\n","      <td>1.947285</td>\n","      <td>2.320965</td>\n","      <td>-0.312589</td>\n","      <td>...</td>\n","      <td>1.901185</td>\n","      <td>0.117700</td>\n","      <td>1.752563</td>\n","      <td>2.015301</td>\n","      <td>0.378365</td>\n","      <td>-0.273318</td>\n","      <td>0.664512</td>\n","      <td>1.629151</td>\n","      <td>-1.360158</td>\n","      <td>-0.709091</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>M</td>\n","      <td>1.704854</td>\n","      <td>2.085134</td>\n","      <td>1.615931</td>\n","      <td>1.723842</td>\n","      <td>0.102458</td>\n","      <td>-0.017833</td>\n","      <td>0.693043</td>\n","      <td>1.263669</td>\n","      <td>-0.217664</td>\n","      <td>...</td>\n","      <td>1.536720</td>\n","      <td>2.047399</td>\n","      <td>1.421940</td>\n","      <td>1.494959</td>\n","      <td>-0.691230</td>\n","      <td>-0.394820</td>\n","      <td>0.236573</td>\n","      <td>0.733827</td>\n","      <td>-0.531855</td>\n","      <td>-0.973978</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>M</td>\n","      <td>0.702284</td>\n","      <td>2.045574</td>\n","      <td>0.672676</td>\n","      <td>0.577953</td>\n","      <td>-0.840484</td>\n","      <td>-0.038680</td>\n","      <td>0.046588</td>\n","      <td>0.105777</td>\n","      <td>-0.809117</td>\n","      <td>...</td>\n","      <td>0.561361</td>\n","      <td>1.374854</td>\n","      <td>0.579001</td>\n","      <td>0.427906</td>\n","      <td>-0.809587</td>\n","      <td>0.350735</td>\n","      <td>0.326767</td>\n","      <td>0.414069</td>\n","      <td>-1.104549</td>\n","      <td>-0.318409</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>M</td>\n","      <td>1.838341</td>\n","      <td>2.336457</td>\n","      <td>1.982524</td>\n","      <td>1.735218</td>\n","      <td>1.525767</td>\n","      <td>3.272144</td>\n","      <td>3.296944</td>\n","      <td>2.658866</td>\n","      <td>2.137194</td>\n","      <td>...</td>\n","      <td>1.961239</td>\n","      <td>2.237926</td>\n","      <td>2.303601</td>\n","      <td>1.653171</td>\n","      <td>1.430427</td>\n","      <td>3.904848</td>\n","      <td>3.197605</td>\n","      <td>2.289985</td>\n","      <td>1.919083</td>\n","      <td>2.219635</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>B</td>\n","      <td>-1.808401</td>\n","      <td>1.221792</td>\n","      <td>-1.814389</td>\n","      <td>-1.347789</td>\n","      <td>-3.112085</td>\n","      <td>-1.150752</td>\n","      <td>-1.114873</td>\n","      <td>-1.261820</td>\n","      <td>-0.820070</td>\n","      <td>...</td>\n","      <td>-1.410893</td>\n","      <td>0.764190</td>\n","      <td>-1.432735</td>\n","      <td>-1.075813</td>\n","      <td>-1.859019</td>\n","      <td>-1.207552</td>\n","      <td>-1.305831</td>\n","      <td>-1.745063</td>\n","      <td>-0.048138</td>\n","      <td>-0.751207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","564         M     2.110995      0.721473        2.060786   2.343856   \n","565         M     1.704854      2.085134        1.615931   1.723842   \n","566         M     0.702284      2.045574        0.672676   0.577953   \n","567         M     1.838341      2.336457        1.982524   1.735218   \n","568         B    -1.808401      1.221792       -1.814389  -1.347789   \n","\n","     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","564         1.041842          0.219060        1.947285             2.320965   \n","565         0.102458         -0.017833        0.693043             1.263669   \n","566        -0.840484         -0.038680        0.046588             0.105777   \n","567         1.525767          3.272144        3.296944             2.658866   \n","568        -3.112085         -1.150752       -1.114873            -1.261820   \n","\n","     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","564      -0.312589  ...      1.901185       0.117700         1.752563   \n","565      -0.217664  ...      1.536720       2.047399         1.421940   \n","566      -0.809117  ...      0.561361       1.374854         0.579001   \n","567       2.137194  ...      1.961239       2.237926         2.303601   \n","568      -0.820070  ...     -1.410893       0.764190        -1.432735   \n","\n","     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","564    2.015301          0.378365          -0.273318         0.664512   \n","565    1.494959         -0.691230          -0.394820         0.236573   \n","566    0.427906         -0.809587           0.350735         0.326767   \n","567    1.653171          1.430427           3.904848         3.197605   \n","568   -1.075813         -1.859019          -1.207552        -1.305831   \n","\n","     concave points_worst  symmetry_worst  fractal_dimension_worst  \n","564              1.629151       -1.360158                -0.709091  \n","565              0.733827       -0.531855                -0.973978  \n","566              0.414069       -1.104549                -0.318409  \n","567              2.289985        1.919083                 2.219635  \n","568             -1.745063       -0.048138                -0.751207  \n","\n","[5 rows x 31 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset.tail()"]},{"cell_type":"markdown","id":"84c80176","metadata":{},"source":["Here, we can see, the first column represents the target variable. The following columns are the features.\n","\n","This is a real-life dataset and before we can apply machine learning algorithms to it, it has to be cleaned and organized. Since we know that machines operate with numbers, we need to convert our target variable from categorical to numerical type. There are many ways to do this. One of the simpler methods is called label encoding. With this, we can convert “M” and “B” to 1 and 0. As a first step, we import LabelEncoder from sklearn library. Then, to make it easier to use, we assign LabelEncoder to the “labelencoder” variable. Finally, we convert the “diagnosis” column from categoric to numeric with the code in the third line. "]},{"cell_type":"code","execution_count":null,"id":"2e583520","metadata":{"id":"2e583520"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","labelencoder = LabelEncoder()\n","dataset[\"diagnosis\"] = labelencoder.fit_transform(dataset[\"diagnosis\"].values) "]},{"cell_type":"code","execution_count":null,"id":"e25a66ff","metadata":{"id":"e25a66ff","outputId":"31d4a744-1316-46eb-8463-10292520b376"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1.097064</td>\n","      <td>-2.073335</td>\n","      <td>1.269934</td>\n","      <td>0.984375</td>\n","      <td>1.568466</td>\n","      <td>3.283515</td>\n","      <td>2.652874</td>\n","      <td>2.532475</td>\n","      <td>2.217515</td>\n","      <td>...</td>\n","      <td>1.886690</td>\n","      <td>-1.359293</td>\n","      <td>2.303601</td>\n","      <td>2.001237</td>\n","      <td>1.307686</td>\n","      <td>2.616665</td>\n","      <td>2.109526</td>\n","      <td>2.296076</td>\n","      <td>2.750622</td>\n","      <td>1.937015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.829821</td>\n","      <td>-0.353632</td>\n","      <td>1.685955</td>\n","      <td>1.908708</td>\n","      <td>-0.826962</td>\n","      <td>-0.487072</td>\n","      <td>-0.023846</td>\n","      <td>0.548144</td>\n","      <td>0.001392</td>\n","      <td>...</td>\n","      <td>1.805927</td>\n","      <td>-0.369203</td>\n","      <td>1.535126</td>\n","      <td>1.890489</td>\n","      <td>-0.375612</td>\n","      <td>-0.430444</td>\n","      <td>-0.146749</td>\n","      <td>1.087084</td>\n","      <td>-0.243890</td>\n","      <td>0.281190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1.579888</td>\n","      <td>0.456187</td>\n","      <td>1.566503</td>\n","      <td>1.558884</td>\n","      <td>0.942210</td>\n","      <td>1.052926</td>\n","      <td>1.363478</td>\n","      <td>2.037231</td>\n","      <td>0.939685</td>\n","      <td>...</td>\n","      <td>1.511870</td>\n","      <td>-0.023974</td>\n","      <td>1.347475</td>\n","      <td>1.456285</td>\n","      <td>0.527407</td>\n","      <td>1.082932</td>\n","      <td>0.854974</td>\n","      <td>1.955000</td>\n","      <td>1.152255</td>\n","      <td>0.201391</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>-0.768909</td>\n","      <td>0.253732</td>\n","      <td>-0.592687</td>\n","      <td>-0.764464</td>\n","      <td>3.283553</td>\n","      <td>3.402909</td>\n","      <td>1.915897</td>\n","      <td>1.451707</td>\n","      <td>2.867383</td>\n","      <td>...</td>\n","      <td>-0.281464</td>\n","      <td>0.133984</td>\n","      <td>-0.249939</td>\n","      <td>-0.550021</td>\n","      <td>3.394275</td>\n","      <td>3.893397</td>\n","      <td>1.989588</td>\n","      <td>2.175786</td>\n","      <td>6.046041</td>\n","      <td>4.935010</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1.750297</td>\n","      <td>-1.151816</td>\n","      <td>1.776573</td>\n","      <td>1.826229</td>\n","      <td>0.280372</td>\n","      <td>0.539340</td>\n","      <td>1.371011</td>\n","      <td>1.428493</td>\n","      <td>-0.009560</td>\n","      <td>...</td>\n","      <td>1.298575</td>\n","      <td>-1.466770</td>\n","      <td>1.338539</td>\n","      <td>1.220724</td>\n","      <td>0.220556</td>\n","      <td>-0.313395</td>\n","      <td>0.613179</td>\n","      <td>0.729259</td>\n","      <td>-0.868353</td>\n","      <td>-0.397100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0          1     1.097064     -2.073335        1.269934   0.984375   \n","1          1     1.829821     -0.353632        1.685955   1.908708   \n","2          1     1.579888      0.456187        1.566503   1.558884   \n","3          1    -0.768909      0.253732       -0.592687  -0.764464   \n","4          1     1.750297     -1.151816        1.776573   1.826229   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0         1.568466          3.283515        2.652874             2.532475   \n","1        -0.826962         -0.487072       -0.023846             0.548144   \n","2         0.942210          1.052926        1.363478             2.037231   \n","3         3.283553          3.402909        1.915897             1.451707   \n","4         0.280372          0.539340        1.371011             1.428493   \n","\n","   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","0       2.217515  ...      1.886690      -1.359293         2.303601   \n","1       0.001392  ...      1.805927      -0.369203         1.535126   \n","2       0.939685  ...      1.511870      -0.023974         1.347475   \n","3       2.867383  ...     -0.281464       0.133984        -0.249939   \n","4      -0.009560  ...      1.298575      -1.466770         1.338539   \n","\n","   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","0    2.001237          1.307686           2.616665         2.109526   \n","1    1.890489         -0.375612          -0.430444        -0.146749   \n","2    1.456285          0.527407           1.082932         0.854974   \n","3   -0.550021          3.394275           3.893397         1.989588   \n","4    1.220724          0.220556          -0.313395         0.613179   \n","\n","   concave points_worst  symmetry_worst  fractal_dimension_worst  \n","0              2.296076        2.750622                 1.937015  \n","1              1.087084       -0.243890                 0.281190  \n","2              1.955000        1.152255                 0.201391  \n","3              2.175786        6.046041                 4.935010  \n","4              0.729259       -0.868353                -0.397100  \n","\n","[5 rows x 31 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"id":"40839716","metadata":{"id":"40839716","outputId":"5eedb3fc-8126-4a1f-cd0a-b4d430e441ed"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>564</th>\n","      <td>1</td>\n","      <td>2.110995</td>\n","      <td>0.721473</td>\n","      <td>2.060786</td>\n","      <td>2.343856</td>\n","      <td>1.041842</td>\n","      <td>0.219060</td>\n","      <td>1.947285</td>\n","      <td>2.320965</td>\n","      <td>-0.312589</td>\n","      <td>...</td>\n","      <td>1.901185</td>\n","      <td>0.117700</td>\n","      <td>1.752563</td>\n","      <td>2.015301</td>\n","      <td>0.378365</td>\n","      <td>-0.273318</td>\n","      <td>0.664512</td>\n","      <td>1.629151</td>\n","      <td>-1.360158</td>\n","      <td>-0.709091</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>1</td>\n","      <td>1.704854</td>\n","      <td>2.085134</td>\n","      <td>1.615931</td>\n","      <td>1.723842</td>\n","      <td>0.102458</td>\n","      <td>-0.017833</td>\n","      <td>0.693043</td>\n","      <td>1.263669</td>\n","      <td>-0.217664</td>\n","      <td>...</td>\n","      <td>1.536720</td>\n","      <td>2.047399</td>\n","      <td>1.421940</td>\n","      <td>1.494959</td>\n","      <td>-0.691230</td>\n","      <td>-0.394820</td>\n","      <td>0.236573</td>\n","      <td>0.733827</td>\n","      <td>-0.531855</td>\n","      <td>-0.973978</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>1</td>\n","      <td>0.702284</td>\n","      <td>2.045574</td>\n","      <td>0.672676</td>\n","      <td>0.577953</td>\n","      <td>-0.840484</td>\n","      <td>-0.038680</td>\n","      <td>0.046588</td>\n","      <td>0.105777</td>\n","      <td>-0.809117</td>\n","      <td>...</td>\n","      <td>0.561361</td>\n","      <td>1.374854</td>\n","      <td>0.579001</td>\n","      <td>0.427906</td>\n","      <td>-0.809587</td>\n","      <td>0.350735</td>\n","      <td>0.326767</td>\n","      <td>0.414069</td>\n","      <td>-1.104549</td>\n","      <td>-0.318409</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>1</td>\n","      <td>1.838341</td>\n","      <td>2.336457</td>\n","      <td>1.982524</td>\n","      <td>1.735218</td>\n","      <td>1.525767</td>\n","      <td>3.272144</td>\n","      <td>3.296944</td>\n","      <td>2.658866</td>\n","      <td>2.137194</td>\n","      <td>...</td>\n","      <td>1.961239</td>\n","      <td>2.237926</td>\n","      <td>2.303601</td>\n","      <td>1.653171</td>\n","      <td>1.430427</td>\n","      <td>3.904848</td>\n","      <td>3.197605</td>\n","      <td>2.289985</td>\n","      <td>1.919083</td>\n","      <td>2.219635</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>0</td>\n","      <td>-1.808401</td>\n","      <td>1.221792</td>\n","      <td>-1.814389</td>\n","      <td>-1.347789</td>\n","      <td>-3.112085</td>\n","      <td>-1.150752</td>\n","      <td>-1.114873</td>\n","      <td>-1.261820</td>\n","      <td>-0.820070</td>\n","      <td>...</td>\n","      <td>-1.410893</td>\n","      <td>0.764190</td>\n","      <td>-1.432735</td>\n","      <td>-1.075813</td>\n","      <td>-1.859019</td>\n","      <td>-1.207552</td>\n","      <td>-1.305831</td>\n","      <td>-1.745063</td>\n","      <td>-0.048138</td>\n","      <td>-0.751207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","564          1     2.110995      0.721473        2.060786   2.343856   \n","565          1     1.704854      2.085134        1.615931   1.723842   \n","566          1     0.702284      2.045574        0.672676   0.577953   \n","567          1     1.838341      2.336457        1.982524   1.735218   \n","568          0    -1.808401      1.221792       -1.814389  -1.347789   \n","\n","     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","564         1.041842          0.219060        1.947285             2.320965   \n","565         0.102458         -0.017833        0.693043             1.263669   \n","566        -0.840484         -0.038680        0.046588             0.105777   \n","567         1.525767          3.272144        3.296944             2.658866   \n","568        -3.112085         -1.150752       -1.114873            -1.261820   \n","\n","     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","564      -0.312589  ...      1.901185       0.117700         1.752563   \n","565      -0.217664  ...      1.536720       2.047399         1.421940   \n","566      -0.809117  ...      0.561361       1.374854         0.579001   \n","567       2.137194  ...      1.961239       2.237926         2.303601   \n","568      -0.820070  ...     -1.410893       0.764190        -1.432735   \n","\n","     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","564    2.015301          0.378365          -0.273318         0.664512   \n","565    1.494959         -0.691230          -0.394820         0.236573   \n","566    0.427906         -0.809587           0.350735         0.326767   \n","567    1.653171          1.430427           3.904848         3.197605   \n","568   -1.075813         -1.859019          -1.207552        -1.305831   \n","\n","     concave points_worst  symmetry_worst  fractal_dimension_worst  \n","564              1.629151       -1.360158                -0.709091  \n","565              0.733827       -0.531855                -0.973978  \n","566              0.414069       -1.104549                -0.318409  \n","567              2.289985        1.919083                 2.219635  \n","568             -1.745063       -0.048138                -0.751207  \n","\n","[5 rows x 31 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dataset.tail()"]},{"cell_type":"markdown","id":"426382a3","metadata":{},"source":["Did you notice that something is missing? Where is our test dataset? This time we have only one file, so we need to divide this dataset into a train set and test set ourselves. We can do that using the sklearn library function train test split."]},{"cell_type":"code","execution_count":null,"id":"781db584","metadata":{"id":"781db584"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"699bbe78","metadata":{"id":"699bbe78"},"outputs":[],"source":["train, test = train_test_split(dataset, test_size=0.3)"]},{"cell_type":"code","execution_count":null,"id":"ae841721","metadata":{"id":"ae841721"},"outputs":[],"source":["X_train = train.drop(\"diagnosis\",axis=1)\n","y_train = train.loc[:,\"diagnosis\"]\n","\n","X_test = test.drop(\"diagnosis\",axis=1)\n","y_test = test.loc[:,\"diagnosis\"]"]},{"cell_type":"markdown","id":"ea7cd83e","metadata":{},"source":["Also, as we did for the regression problem, we need to define the “target” we want to predict. In this problem, we’re trying to predict if the tumor is malignant (1) or benign (0). Hence, our target variable is the “diagnosis” column. And the rest of the columns are “features”. Let’s assign the x variable as target and the y variable as features. And remember, we need to do this for both the train and test datasets. By the way, you can also do this for the whole dataset and then divide into train and test. It’s up to you. We’re ready to import our logistic regression model from sklearn library. And after importing the logistic regression model, we can assign it to the “model” variable. Now, we’re ready to train our model, that means teach the hidden patterns in the train dataset to our model. And finally, we can make the predictions on the test dataset."]},{"cell_type":"code","execution_count":null,"id":"e0f1935f","metadata":{"id":"e0f1935f"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"id":"86ab8163","metadata":{"id":"86ab8163"},"outputs":[],"source":["model_1 = LogisticRegression()"]},{"cell_type":"code","execution_count":null,"id":"744c283d","metadata":{"id":"744c283d","outputId":"b98b52cc-dfb4-47ac-be31-6f39a10f97dd"},"outputs":[{"data":{"text/plain":["LogisticRegression()"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model_1.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"id":"7c79e90d","metadata":{"id":"7c79e90d","outputId":"3fa2dd35-196e-4774-ea4a-bb1d5c3ec04e"},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n","       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["predictions = model_1.predict(X_test)\n","predictions"]},{"cell_type":"markdown","id":"501e2bce","metadata":{},"source":["Using the confusion matrix, we can check the accuracy of our results. First, we import confusion_matrix from sklearn and display the number of each metric. We have 103 true negatives, 0 false positives, 4 false negatives, 64 true positives. That means out of 171 predictions are correct, isn’t it amazing? Let’s continue. We import classification_report from sklearn and display the evaluation metrics. And the ratios we get, are quite high. We’ve done a really good job."]},{"cell_type":"code","execution_count":null,"id":"a1840540","metadata":{"id":"a1840540","outputId":"8090f709-f040-4055-a014-dc2d8e76cc97"},"outputs":[{"data":{"text/plain":["array([[101,   0],\n","       [  6,  64]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_test, predictions)"]},{"cell_type":"code","execution_count":null,"id":"cbfed89a","metadata":{"id":"cbfed89a","outputId":"697d2ba3-2db9-4220-aeb2-f756e3193187"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97       101\n","           1       1.00      0.91      0.96        70\n","\n","    accuracy                           0.96       171\n","   macro avg       0.97      0.96      0.96       171\n","weighted avg       0.97      0.96      0.96       171\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, predictions))"]},{"cell_type":"markdown","id":"a0251181","metadata":{},"source":["Well done! We have actually trained and tested a logistic regression classifier. Now, why not try another classification algorithm: Support Vector Machine. SVM is a supervised machine learning technique that can be used to solve classification and regression problems. It is, however, mostly used for classification. In this algorithm, we use an axis to represent each feature and plot all data points in the space. Then, the SVM model finds boundaries to separate these classes. The decision boundary is what separates different data samples into specific classes. Consider a dataset of different animals of two classes: birds and fish. In this dataset there are only three features: body weight, body length, and daily food consumption. We draw a 3-dimensional grid and plot all these points. A SVM model will try to find a 2D plane that differentiates the 2 classes."]},{"cell_type":"code","execution_count":null,"id":"4502b446","metadata":{"id":"4502b446"},"outputs":[],"source":["from sklearn.svm import LinearSVC"]},{"cell_type":"code","execution_count":null,"id":"0c1962fc","metadata":{"id":"0c1962fc"},"outputs":[],"source":["model_2 = LinearSVC()"]},{"cell_type":"code","execution_count":null,"id":"d0e0aaa5","metadata":{"id":"d0e0aaa5","outputId":"475b2d21-9600-4b28-cdef-df3db503f666"},"outputs":[{"data":{"text/plain":["LinearSVC()"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model_2.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"id":"06368e44","metadata":{"id":"06368e44","outputId":"b4e4cd80-09af-4a24-8527-ff7e75b79544"},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n","       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["predictions = model_2.predict(X_test)\n","predictions"]},{"cell_type":"code","execution_count":null,"id":"59a01cb3","metadata":{"id":"59a01cb3","outputId":"3757ea3e-60e8-438c-e606-4a3817d777f2"},"outputs":[{"data":{"text/plain":["array([[101,   0],\n","       [  7,  63]])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_test, predictions)"]},{"cell_type":"code","execution_count":null,"id":"b699c1c0","metadata":{"id":"b699c1c0","outputId":"49740443-48b5-44ba-83b8-81295b5d37f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97       101\n","           1       1.00      0.90      0.95        70\n","\n","    accuracy                           0.96       171\n","   macro avg       0.97      0.95      0.96       171\n","weighted avg       0.96      0.96      0.96       171\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, predictions))"]},{"cell_type":"markdown","id":"146a87d3","metadata":{},"source":["If there were more than 3 features, we would have a hyper-space. A hyper-space is a space with higher than 3 dimensions like 4D, 5D, and so on and therefore it is not possible to visualize. We can find a hyper-plane that clearly distinguishes different classes. Hyper-planes are multidimensional planes that exist in four or more dimensions. This hyper-plane is used as a condition to perform classification. If the hyper-planes are linear, the SVM is called Linear Kernel SVM. However, the hyper-plane can be nonlinear as well. In that case we use a Polynomial Kernel or other advanced SVMs. Let’s see how this model performs with the same breast cancer dataset we used earlier. We start with importing LinearSVC from sklearn and assigning it to the variable. Now, we’re ready to train our model, that means teach the hidden patterns in the train dataset to our model. Finally, we can make the predictions on the test dataset.\n","\n","Our predictions with the Support Vector Classifier are ready! Now, we can check the accuracy of our model in the same way we did for Logistic Regression. We can start with the confusion matrix. We have one hundred one true negatives, 2 false positives, 4 false negatives, 64 true positives. This means that 165 out of 171 predictions are correct, just a couple less than before. We should also check the classification report. We get quite high metrics here as well. But we got better with Logistic Regression. And here our false positives were higher. This is a key metric for this dataset we want to minimize, because we don’t want healthy patients to be diagnosed with cancer. Therefore, we prefer using the Logistic Regression model for this problem and dataset."]},{"cell_type":"markdown","id":"ab698209","metadata":{},"source":["The confusion matrix is a table that is used to evaluate the performance of a classification model. It is a square matrix with two dimensions: the predicted class and the actual class. The entries in the matrix represent the number of instances that were predicted to be in a particular class and were actually in that class.\n","The formula for the classification evaluation metric is:\n","\n","Accuracy = (TP + TN) / (TP + TN + FP + FN)\n","\n","where:\n","* TP = true positives\n","* TN = true negatives\n","* FP = false positives\n","* FN = false negatives\n","Accuracy is the proportion of instances that were correctly classified. It is a measure of how well the model can distinguish between the different classes."]},{"cell_type":"markdown","id":"9c320310","metadata":{},"source":["In the confusion matrix, we want to maximize the true positives (TP) and true negatives (TN) and minimize the false positives (FP) and false negatives (FN).\n","TP represents the number of instances that were correctly classified as positive. TN represents the number of instances that were correctly classified as negative. FP represents the number of instances that were incorrectly classified as positive. FN represents the number of instances that were incorrectly classified as negative.\n","Maximizing TP and TN will increase the accuracy of the model, while minimizing FP and FN will reduce the error rate."]},{"cell_type":"markdown","id":"67d6e7a9","metadata":{},"source":["Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. One application of the logistic regression algorithm is to solve binary classification problems, such as predicting spam or non-spam mails.\n","\n","The sigmoid function takes any real input and outputs a value between zero and one. In logistic regression, the ​sigmoid function is used to solve classification problems. Its shape in a plane resembles the letter S.\n","\n","The recall metric provides information about the model's ability to predict positive samples. It is defined as the ratio of a correctly classified positive class (TP) divided by the total number of instances which are actually positive (TP+FN).\n","\n","We want our true positives and negatives to be maximized and false positives and negatives to be minimized.\n","\n","Classification is the process of categorizing a given set of data into classes. Types of problems with two class labels are called binary classification. Unlike linear regression, logistic regression does not need a linear relationship between input and output variables. In the confusion matrix, we want our true positives and negatives maximized and false positives and negatives minimized.\n","\n","Hyper-planes are multidimensional planes that exist in four or more dimensions. If we try to solve a classification problem with more than three features using the SVM algorithm, it uses a hyper-plane that clearly distinguishes the different classes.\n","\n","If the hyper-planes are linear, the SVM is called linear kernel SVM. Polynomial kernel or other advanced SVMs used for non-linear hyper-planes. The F1 score is also known as the F measure. It indicates the equilibrium between precision and recall.\n","\n","Binary classification refers to classification tasks that have two class labels. Spam detection is an example of binary classification where the output is restricted to two labels, spam or non-spam.\n","\n","If only one dataset is available, it needs to be split into train and test data. To do this, the train_test_split() function from sklearn can be used.\n","\n","The support vector machine algorithm (SVM) can be used for classification and regression problems, but mostly for classification problems. SVM uses decision boundaries to separate classes. In the data, the target variable needs to be converted to a numerical type if it is categorical. SVM algorithm uses hyper-planes if there are more than three features. So, a hyper-plane is a space with higher than three dimensions."]}],"metadata":{"colab":{"name":"Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
