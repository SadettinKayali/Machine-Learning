{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xibu6qo7pONV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tb6n-Ma_zXGj"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UAp0ISkJzezQ"
      },
      "outputs": [],
      "source": [
        "X = data.drop('SalePrice',axis=1)\n",
        "y = data.loc[:,'SalePrice']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HTqqAoiYrTyE"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C2l--kjHqty6"
      },
      "outputs": [],
      "source": [
        "linear_reg = LinearRegression()\n",
        "ridge_reg = Ridge(alpha=0.05, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG30YQkKq3rY",
        "outputId": "ff4b43f7-c192-455d-c18c-bcc631c2cc99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sadet\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Ridge(alpha=0.05, normalize=True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_reg.fit(X_train, y_train)\n",
        "ridge_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P_WIcc1wtDuu"
      },
      "outputs": [],
      "source": [
        "linear_pred = linear_reg.predict(X_test)\n",
        "ridge_pred = ridge_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ciejY2e4t7dx"
      },
      "outputs": [],
      "source": [
        "linear_mse = mean_squared_error(y_test, linear_pred)\n",
        "ridge_mse = mean_squared_error(y_test, ridge_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7u6r3cgu3h5",
        "outputId": "cec4d509-6b09-4aa0-d228-c3dacbe33ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE without Ridge: 5116399803.951063\n",
            "MSE with Ridge : 4465770299.03716\n"
          ]
        }
      ],
      "source": [
        "print(f\"MSE without Ridge: {linear_mse}\")\n",
        "print(f\"MSE with Ridge : {ridge_mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Underfitting describes the problem of a model being too simple so that it is unable to find the patterns in the training dataset. It does not fit the data properly, and ignores a large portion of it.\n",
        "\n",
        "Overfitting describes the problem of a model being too specific for a dataset and trying to fit every datapoint. The model is unable to generalize for other data because it is looking for the specific patterns in the training dataset.\n",
        "\n",
        "Variance refers to the sensitivity of a model to specific datasets. The variance is high in the case of overfitting but low for underfitting. Bias, on the other hand, refers to the inability of the model to understand the complexity of data. The bias is high when a model is an underfit but low for an overfit.\n",
        "\n",
        "There is a trade-off between bias and variance. This means that as variance increases, bias decreases and vice versa. For a good-performing model, there needs to be a good balance.\n",
        "\n",
        "The tuning parameter lambda(λ) is used to specify how much we want to penalize the flexibility of our model. It helps shrink the less predictive features’ coefficient.\n",
        "\n",
        "L1 regularization is equal to the absolute value of the magnitude of the coefficient; it simply restricts or penalizes the size of the coefficients. In the L2 loss function, the magnitude of coefficients is squared. When there are outliers in the dataset, using the L2 loss function is not useful because taking squares of the differences between the actual and predicted values will lead to a much larger error, while the L1 loss function is not affected by them.\n",
        "\n",
        "The general idea for solving overfitting and high variance is to make the data less complex. Regularization prevents the learning of more complex patterns.\n",
        "\n",
        "When there are outliers in the dataset, using the L2 loss function is not useful because taking squares of the differences between the actual and predicted values will lead to a much larger error.\n",
        "\n",
        "The loss function L2 is used in ridge regression. The L1 loss function is used in lasso regression.\n",
        "\n",
        "The variance refers to the sensitivity of a model to specific datasets. The variance is high in the case of overfitting and low in the case of underfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Regularization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
